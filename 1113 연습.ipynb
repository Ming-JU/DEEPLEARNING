{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('hello,tensorflow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello,tensorflow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a,b,c]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n",
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== w ===\n",
      "[[ 0.3541501   1.3858387 ]\n",
      " [-0.85082334 -1.1777186 ]\n",
      " [ 0.48531252 -0.2150921 ]]\n",
      "=== b ===\n",
      "[[-0.3493833 ]\n",
      " [ 0.24323048]]\n",
      "=== expr ===\n",
      "[[-0.2409423 -1.9642582]\n",
      " [ 0.3175894 -1.3925604]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,[None,3])\n",
    "print(X)\n",
    "\n",
    "x_data = [[1,2,3],[4,5,6]]\n",
    "w = tf.Variable(tf.random_normal([3,2]))\n",
    "b = tf.Variable(tf.random_normal([2,1]))\n",
    "\n",
    "expr = tf.matmul(X,w) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('=== x_data ===')\n",
    "print(x_data)\n",
    "print('=== w ===')\n",
    "print(sess.run(w))\n",
    "print('=== b ===')\n",
    "print(sess.run(b))\n",
    "print('=== expr ===')\n",
    "\n",
    "print(sess.run(expr, feed_dict= {X: x_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.096667 [0.9666668] [0.36000004]\n",
      "1 0.08678543 [0.8537777] [0.30133328]\n",
      "2 0.014332983 [0.86971855] [0.29955554]\n",
      "3 0.01283593 [0.8714923] [0.29175702]\n",
      "4 0.0122164665 [0.87473] [0.2848087]\n",
      "5 0.011636075 [0.87772524] [0.27795497]\n",
      "6 0.011083321 [0.8806663] [0.27127385]\n",
      "7 0.010556861 [0.88353485] [0.26475254]\n",
      "8 0.01005541 [0.88633466] [0.2583881]\n",
      "9 0.009577766 [0.8890671] [0.25217664]\n",
      "10 0.009122823 [0.8917339] [0.24611448]\n",
      "11 0.008689478 [0.89433646] [0.24019803]\n",
      "12 0.008276716 [0.8968765] [0.23442383]\n",
      "13 0.00788357 [0.8993556] [0.22878847]\n",
      "14 0.0075091007 [0.901775] [0.22328855]\n",
      "15 0.0071524014 [0.90413624] [0.21792084]\n",
      "16 0.006812664 [0.90644073] [0.21268217]\n",
      "17 0.0064890515 [0.9086898] [0.20756944]\n",
      "18 0.006180823 [0.91088486] [0.20257963]\n",
      "19 0.0058872257 [0.9130271] [0.19770975]\n",
      "20 0.005607575 [0.9151179] [0.19295695]\n",
      "21 0.0053412244 [0.9171584] [0.18831839]\n",
      "22 0.0050875014 [0.9191498] [0.18379132]\n",
      "23 0.004845845 [0.92109346] [0.17937313]\n",
      "24 0.0046156673 [0.9229903] [0.17506114]\n",
      "25 0.004396413 [0.9248415] [0.17085277]\n",
      "26 0.0041875825 [0.9266483] [0.1667456]\n",
      "27 0.0039886697 [0.92841166] [0.16273716]\n",
      "28 0.0037992122 [0.93013257] [0.15882505]\n",
      "29 0.0036187416 [0.93181217] [0.155007]\n",
      "30 0.0034468565 [0.93345135] [0.15128075]\n",
      "31 0.0032831226 [0.93505114] [0.14764407]\n",
      "32 0.0031271747 [0.9366124] [0.1440948]\n",
      "33 0.002978626 [0.9381362] [0.14063087]\n",
      "34 0.0028371334 [0.93962336] [0.13725019]\n",
      "35 0.0027023738 [0.9410748] [0.1339508]\n",
      "36 0.0025740124 [0.94249135] [0.13073073]\n",
      "37 0.002451741 [0.9438738] [0.12758805]\n",
      "38 0.0023352874 [0.94522303] [0.12452091]\n",
      "39 0.0022243552 [0.9465398] [0.12152751]\n",
      "40 0.0021186993 [0.947825] [0.11860608]\n",
      "41 0.002018051 [0.9490792] [0.11575487]\n",
      "42 0.0019222018 [0.9503033] [0.1129722]\n",
      "43 0.0018308932 [0.951498] [0.11025642]\n",
      "44 0.0017439212 [0.952664] [0.10760596]\n",
      "45 0.0016610817 [0.9538019] [0.10501915]\n",
      "46 0.0015821807 [0.9549125] [0.10249458]\n",
      "47 0.0015070267 [0.95599633] [0.10003068]\n",
      "48 0.0014354441 [0.95705414] [0.09762599]\n",
      "49 0.0013672573 [0.95808655] [0.09527914]\n",
      "50 0.0013023095 [0.9590941] [0.09298868]\n",
      "51 0.001240448 [0.96007746] [0.0907533]\n",
      "52 0.0011815274 [0.9610372] [0.08857167]\n",
      "53 0.0011254038 [0.9619738] [0.08644243]\n",
      "54 0.0010719461 [0.9628879] [0.08436441]\n",
      "55 0.0010210272 [0.96378005] [0.08233637]\n",
      "56 0.000972529 [0.96465075] [0.08035706]\n",
      "57 0.0009263323 [0.96550053] [0.07842534]\n",
      "58 0.0008823324 [0.96632993] [0.07654006]\n",
      "59 0.000840425 [0.96713936] [0.07470009]\n",
      "60 0.0008004987 [0.96792924] [0.07290431]\n",
      "61 0.00076247816 [0.96870023] [0.07115175]\n",
      "62 0.00072625844 [0.9694526] [0.0694413]\n",
      "63 0.00069176004 [0.970187] [0.06777199]\n",
      "64 0.0006589004 [0.9709037] [0.06614278]\n",
      "65 0.0006275992 [0.97160316] [0.06455275]\n",
      "66 0.00059779076 [0.97228575] [0.06300093]\n",
      "67 0.0005693946 [0.972952] [0.06148645]\n",
      "68 0.00054234714 [0.9736022] [0.06000834]\n",
      "69 0.0005165871 [0.97423685] [0.05856582]\n",
      "70 0.0004920501 [0.97485614] [0.05715792]\n",
      "71 0.0004686739 [0.9754605] [0.05578387]\n",
      "72 0.000446414 [0.97605056] [0.0544429]\n",
      "73 0.00042521022 [0.9766262] [0.05313409]\n",
      "74 0.00040500925 [0.97718805] [0.05185677]\n",
      "75 0.00038577418 [0.9777365] [0.05061019]\n",
      "76 0.00036744677 [0.97827166] [0.04939355]\n",
      "77 0.0003499939 [0.97879404] [0.04820617]\n",
      "78 0.00033336974 [0.97930384] [0.04704733]\n",
      "79 0.0003175337 [0.9798013] [0.04591632]\n",
      "80 0.00030244896 [0.9802869] [0.04481253]\n",
      "81 0.00028808333 [0.9807608] [0.04373527]\n",
      "82 0.00027439915 [0.9812233] [0.04268389]\n",
      "83 0.00026136634 [0.9816747] [0.0416578]\n",
      "84 0.00024895064 [0.9821152] [0.04065638]\n",
      "85 0.00023712592 [0.98254514] [0.03967903]\n",
      "86 0.00022586128 [0.9829647] [0.03872516]\n",
      "87 0.0002151341 [0.98337424] [0.03779425]\n",
      "88 0.00020491196 [0.9837739] [0.03688569]\n",
      "89 0.00019517982 [0.984164] [0.035999]\n",
      "90 0.00018590938 [0.98454463] [0.03513359]\n",
      "91 0.00017707905 [0.9849162] [0.03428902]\n",
      "92 0.00016866605 [0.9852788] [0.03346473]\n",
      "93 0.00016065601 [0.9856327] [0.03266027]\n",
      "94 0.00015302414 [0.98597807] [0.03187511]\n",
      "95 0.00014575441 [0.9863152] [0.03110888]\n",
      "96 0.00013883262 [0.98664415] [0.03036102]\n",
      "97 0.00013223822 [0.98696524] [0.02963117]\n",
      "98 0.00012595567 [0.9872785] [0.02891883]\n",
      "99 0.00011997231 [0.98758435] [0.02822366]\n",
      "====Test====\n",
      "X:5, Y: [4.966145]\n",
      "X:2.5, Y: [2.4971845]\n"
     ]
    }
   ],
   "source": [
    "X_data = [1,2,3]\n",
    "Y_data = [1,2,3]\n",
    "W = tf.Variable(tf.random_uniform([1],-0.1,-0.1))\n",
    "b = tf.Variable(tf.random_uniform([1],-0.1,-0.1))\n",
    "\n",
    "X = tf.placeholder(tf.float32,name = 'X')\n",
    "Y = tf.placeholder(tf.float32,name = 'Y')\n",
    "\n",
    "hypothesis = W*X+b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op,cost],feed_dict = {X:X_data,Y:Y_data})\n",
    "        print(step,cost_val,sess.run(W),sess.run(b))\n",
    "        \n",
    "    print('====Test====')\n",
    "    print('X:5, Y:',sess.run(hypothesis,feed_dict = {X:5}))\n",
    "    print('X:2.5, Y:',sess.run(hypothesis,feed_dict = {X:2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-5d2f99778a0f>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-8-5d2f99778a0f>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax_cross_entropy_with_logits() got an unexpected keyword argument 'lables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5d2f99778a0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initizlizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m               instructions)\n\u001b[1;32m--> 306\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax_cross_entropy_with_logits() got an unexpected keyword argument 'lables'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot = True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "keep_porf = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256], stddev = 0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1))\n",
    "L1 = tf.nn.dropout(L1,0.8)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256], stddev = 0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2))\n",
    "L2 = tf.nn.dropout(L2,0.8)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10], stddev = 0.01))\n",
    "model = tf.matmul(L2,W3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model,lables=Y))\n",
    "\n",
    "init = tf.global_variables_initizlizer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys,keep_prob:0.8})\n",
    "        \n",
    "        print('Epoch:','$04d' % (epoch+1), 'Avg. cost=', '{:.3f}'.format(total_cost / total_batch ))\n",
    "        \n",
    "print('최적화 완료!')\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "epoch : 0001 Avg. cost= 0.443\n",
      "epoch : 0002 Avg. cost= 0.172\n",
      "epoch : 0003 Avg. cost= 0.118\n",
      "epoch : 0004 Avg. cost= 0.092\n",
      "epoch : 0005 Avg. cost= 0.076\n",
      "Learin Finish!\n",
      "정확도 :  0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('./mnist/data/',one_hot=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X=tf.placeholder(tf.float32,[None,784])\n",
    "Y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32) #Drop out 위해 필요?\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([784,256],stddev=0.01))\n",
    "L1=tf.nn.relu(tf.matmul(X,W1))\n",
    "L1=tf.nn.dropout(L1,0.8)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([256,256],stddev=0.01))\n",
    "L2=tf.nn.relu(tf.matmul(L1,W2))\n",
    "L2=tf.nn.dropout(L2,0.8)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "model=tf.matmul(L2,W3)\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model,labels=Y))\n",
    "optimizer=tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size=100\n",
    "total_batch=int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_cost=0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        _,cost_val=sess.run([optimizer,cost],feed_dict={X:batch_xs,Y:batch_ys,keep_prob:0.8})\n",
    "        total_cost+=cost_val/total_batch\n",
    "    print(\"epoch :\",\"%04d\"%(epoch+1),'Avg. cost=','{:.3f}'.format(total_cost))\n",
    "print(\"Learin Finish!\")\n",
    "\n",
    "is_correct=tf.equal(tf.argmax(model,1),tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "print(\"정확도 : \", sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1}))\n",
    "\n",
    "labels=sess.run(model,feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_prob:1})\n",
    "\n",
    "fig=plt.figure()\n",
    "for i in range(10):\n",
    "    subplot=fig.add_subplot(2,5,i+1)\n",
    "    subplot.set_xticks([])\n",
    "    subplot.set_yticks([])\n",
    "    subplot.set_title(\"%d\"%np.argmax(labels[i]))\n",
    "    subplot.imshow(mnist.test.images[i].reshape((28,28)),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
